{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnUGuiBtcdVUN3LxgVI81s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/NLP/blob/main/NextWord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_md --q\n",
        "# !pip install gensim --q"
      ],
      "metadata": {
        "id": "b8kOtbEVcdpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "0bAG0BX-s3WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"You\", \"can\", \"use\", \"Word2Vec\", \"for\", \"word\", \"embeddings\"],\n",
        "    [\"Machine\", \"learning\", \"is\", \"fun\"],\n",
        "    [\"Deep\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"machine\", \"learning\"]\n",
        "]\n",
        "\n",
        "sentences = [[w.lower() for w in s] for s in sentences]"
      ],
      "metadata": {
        "id": "PKOmIKlxt2pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab (shift indices so 1..N, padding = 0)\n",
        "vocab_words = sorted(set(word for sent in sentences for word in sent))\n",
        "old_vocab = {w: i for i, w in enumerate(vocab_words)}\n",
        "vocab = {w: i + 1 for w, i in old_vocab.items()}   # shift\n",
        "\n",
        "idx2word = {i: w for w, i in vocab.items()}        # reverse map"
      ],
      "metadata": {
        "id": "DmfXNyqat7aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy vectors & build embedding matrix\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "embed_dim = nlp.vocab.vectors_length\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "mWtTtEitt_bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size + 1, embed_dim))\n",
        "for word, idx in vocab.items():\n",
        "    embedding_matrix[idx] = nlp.vocab[word].vector\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "3tFy0jcDuCPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for sent in sentences:\n",
        "    indices = [vocab[w] for w in sent]\n",
        "    for i in range(1, len(indices)):       # start from 1\n",
        "        train_X.append(indices[:i])        # prefix\n",
        "        train_y.append(indices[i])         # next word"
      ],
      "metadata": {
        "id": "G28PkjWhuJwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = torch.tensor(self.X[idx], dtype=torch.long)\n",
        "        target = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return seq, target"
      ],
      "metadata": {
        "id": "dhE-CZbCuPDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    seqs, targets = zip(*batch)\n",
        "    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
        "    targets = torch.tensor(targets)\n",
        "    return padded, targets\n",
        "\n",
        "dataset = NextWordDataset(train_X, train_y)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "jjzqMmreuSgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Language Model\n",
        "class NextWordLSTM(nn.Module):\n",
        "    def __init__(self, emb_matrix, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        vocab_size, embed_dim = emb_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(emb_matrix)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        out, (h, c) = self.lstm(emb)\n",
        "        last_hidden = out[:, -1, :]           # last timestep\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "QyjAFLVRuYEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NextWordLSTM(embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "yefvor8TuaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    for batch_x, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_x)\n",
        "        loss = criterion(logits, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-fJbPwAufuK",
        "outputId": "c8dc2c8f-457a-47f7-b69a-ff7533b3a0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 3.0613\n",
            "Epoch 2 Loss: 2.9622\n",
            "Epoch 3 Loss: 2.8323\n",
            "Epoch 4 Loss: 2.8853\n",
            "Epoch 5 Loss: 2.8102\n",
            "Epoch 6 Loss: 2.7702\n",
            "Epoch 7 Loss: 2.3576\n",
            "Epoch 8 Loss: 2.1370\n",
            "Epoch 9 Loss: 2.2021\n",
            "Epoch 10 Loss: 2.1496\n",
            "Epoch 11 Loss: 1.6133\n",
            "Epoch 12 Loss: 1.8841\n",
            "Epoch 13 Loss: 1.7137\n",
            "Epoch 14 Loss: 0.9329\n",
            "Epoch 15 Loss: 1.1575\n",
            "Epoch 16 Loss: 1.3580\n",
            "Epoch 17 Loss: 0.6462\n",
            "Epoch 18 Loss: 0.4881\n",
            "Epoch 19 Loss: 0.7325\n",
            "Epoch 20 Loss: 0.2913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, prefix_words):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ids = [vocab.get(w.lower(), 0) for w in prefix_words]\n",
        "        x = torch.tensor(ids).unsqueeze(0)\n",
        "        logits = model(x)\n",
        "        next_id = torch.argmax(logits, dim=1).item()\n",
        "        return idx2word.get(next_id, \"<unk>\")"
      ],
      "metadata": {
        "id": "lEx9Hdxxuk3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it\n",
        "print(\"\\nNext word after: ['machine', 'learning', 'is'] →\")\n",
        "print(\"Prediction:\", predict_next_word(model, [\"machine\", \"learning\", \"is\"]))\n",
        "\n",
        "print(\"\\nNext word after: ['i', 'love'] →\")\n",
        "print(\"Prediction:\", predict_next_word(model, [\"i\", \"love\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSnnxc_1slBT",
        "outputId": "02b0b68e-d7e6-464d-c479-3bc8cf915309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Next word after: ['machine', 'learning', 'is'] →\n",
            "Prediction: fun\n",
            "\n",
            "Next word after: ['i', 'love'] →\n",
            "Prediction: natural\n"
          ]
        }
      ]
    }
  ]
}